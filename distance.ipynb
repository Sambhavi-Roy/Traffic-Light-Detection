{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Sambhavi Roy/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-5-29 Python-3.11.9 torch-2.0.0+cu118 CUDA:0 (NVIDIA GeForce GTX 1650, 4096MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7018216 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "\n",
    "# Load YOLOv5 model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path=r\"C:\\Users\\Sambhavi Roy\\Downloads\\cinTA_v2.v1-bismillah-ya-rabbalamin.yolov8\\yolov5-master\\yolov5-master\\runs\\train\\exp4\\weights\\best.pt\")\n",
    "\n",
    "# Load the video feed from the dashcam\n",
    "cap = cv2.VideoCapture(r\"C:\\Users\\Sambhavi Roy\\Downloads\\Traffic Lights UK - Traffic Lights Driving Lesson (online-video-cutter.com).mp4\")\n",
    "\n",
    "# Known parameters (adjust these values based on real-world camera settings)\n",
    "KNOWN_HEIGHT = 0.6  # height of the traffic light in meters (example)\n",
    "FOCAL_LENGTH = 3.1 * 1000 / 1920  # focal length in pixels (example, needs calibration)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Perform inference on the current frame\n",
    "    results = model(frame)\n",
    "\n",
    "    # Render the detections on the frame\n",
    "    annotated_frame = results.render()[0].copy()  # Make a copy of the rendered frame\n",
    "\n",
    "    # Loop through the detections\n",
    "    for *box, conf, cls in results.xyxy[0]:  # xyxy format for bounding box\n",
    "        x_min, y_min, x_max, y_max = map(int, box)  # Convert to integer\n",
    "        height_in_pixels = y_max - y_min  # Height of bounding box in pixels\n",
    "\n",
    "        # Calculate the distance\n",
    "        if height_in_pixels > 0:  # Avoid division by zero\n",
    "            distance = (KNOWN_HEIGHT * FOCAL_LENGTH) / height_in_pixels\n",
    "            distance_text = f\"Distance: {distance:.2f} m\"\n",
    "        else:\n",
    "            distance_text = \"Distance: N/A\"\n",
    "\n",
    "        # Draw the distance text on the copied rendered frame\n",
    "        cv2.putText(annotated_frame, distance_text, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "\n",
    "    # Display the frame with YOLOv5 detections and distance estimations\n",
    "    cv2.imshow('Dashcam Traffic Light Detection and Distance Estimation', annotated_frame)\n",
    "\n",
    "    # Exit the video display when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release video capture and close display windows   \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "meenambakkam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Sambhavi Roy/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-5-29 Python-3.11.9 torch-2.0.0+cu118 CUDA:0 (NVIDIA GeForce GTX 1650, 4096MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7018216 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Width: 3840.0\n",
      "Frame Height: 2160.0\n",
      "Total Frames: 1537.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "\n",
    "# Load YOLOv5 model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path=r\"C:\\Users\\Sambhavi Roy\\Downloads\\cinTA_v2.v1-bismillah-ya-rabbalamin.yolov8\\yolov5-master\\yolov5-master\\runs\\train\\exp4\\weights\\best.pt\")\n",
    "\n",
    "# Load the video feed from the dashcam\n",
    "cap = cv2.VideoCapture(r\"C:\\Users\\Sambhavi Roy\\Downloads\\S3 Meenambakkam signal1.mp4\")\n",
    "\n",
    "# Check video capture properties\n",
    "print(f\"Frame Width: {cap.get(cv2.CAP_PROP_FRAME_WIDTH)}\")\n",
    "print(f\"Frame Height: {cap.get(cv2.CAP_PROP_FRAME_HEIGHT)}\")\n",
    "print(f\"Total Frames: {cap.get(cv2.CAP_PROP_FRAME_COUNT)}\")\n",
    "\n",
    "# Known parameters (you need to adjust these values)\n",
    "KNOWN_HEIGHT = 0.6  # height of the traffic light in meters (example)\n",
    "FOCAL_LENGTH = 2623  # Convert focal length to pixels (approx, adjust based on your setup)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Display the original frame\n",
    "    cv2.imshow('Original Frame', frame)  # Show the original frame for debugging\n",
    "\n",
    "    # Resize frame if necessary\n",
    "    frame = cv2.resize(frame, (640, 480))  # Resize to a standard dimension\n",
    "\n",
    "    # Perform inference on the current frame\n",
    "    results = model(frame)\n",
    "\n",
    "    # Loop through the detections\n",
    "    for *box, conf, cls in results.xyxy[0]:  # xyxy format for bounding box\n",
    "        x_min, y_min, x_max, y_max = map(int, box)  # Convert to integer\n",
    "        height_in_pixels = y_max - y_min  # Height of bounding box in pixels\n",
    "\n",
    "        # Calculate the distance\n",
    "        if height_in_pixels > 0:  # Avoid division by zero\n",
    "            distance = (KNOWN_HEIGHT * FOCAL_LENGTH) / height_in_pixels\n",
    "            distance_text = f\"Distance: {distance:.2f} m\"\n",
    "        else:\n",
    "            distance_text = \"Distance: N/A\"\n",
    "\n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (255, 0, 0), 2)  # Bounding box\n",
    "\n",
    "        # Calculate the position for distance text\n",
    "        text_x = x_min\n",
    "        text_y = y_min - 20  # Position above the bounding box\n",
    "\n",
    "        # Draw a background rectangle for the distance text\n",
    "        text_width, text_height = cv2.getTextSize(distance_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]\n",
    "        cv2.rectangle(frame, (text_x, text_y - text_height), (text_x + text_width, text_y + 5), (255, 255, 255), -1)  # White background for text\n",
    "\n",
    "        # Draw the distance text\n",
    "        cv2.putText(frame, distance_text, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "\n",
    "    # Render the detections on the frame\n",
    "    annotated_frame = results.render()[0]  # Rendered frame with bounding boxes and labels\n",
    "\n",
    "    # Combine annotated frame with the original frame for display\n",
    "    combined_frame = cv2.addWeighted(frame, 0.7, annotated_frame, 0.3, 0)\n",
    "\n",
    "    # Display the frame with YOLOv5 detections and distance estimations\n",
    "    cv2.imshow('Dashcam Traffic Light Detection and Distance Estimation', combined_frame)\n",
    "\n",
    "    # Exit the video display when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release video capture and close display windows   \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50-55m at 20kmph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving output as video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Sambhavi Roy/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-5-29 Python-3.11.9 torch-2.0.0+cu118 CUDA:0 (NVIDIA GeForce GTX 1650, 4096MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7018216 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Width: 3840.0\n",
      "Frame Height: 2160.0\n",
      "Total Frames: 1526.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "\n",
    "# Load YOLOv5 model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path=r\"C:\\Users\\Sambhavi Roy\\Downloads\\cinTA_v2.v1-bismillah-ya-rabbalamin.yolov8\\yolov5-master\\yolov5-master\\runs\\train\\exp4\\weights\\best.pt\")\n",
    "\n",
    "# Load the video feed from the dashcam\n",
    "cap = cv2.VideoCapture(r\"C:\\Users\\Sambhavi Roy\\Downloads\\H1538_1725962886.mp4\")\n",
    "\n",
    "# Check video capture properties\n",
    "print(f\"Frame Width: {cap.get(cv2.CAP_PROP_FRAME_WIDTH)}\")\n",
    "print(f\"Frame Height: {cap.get(cv2.CAP_PROP_FRAME_HEIGHT)}\")\n",
    "print(f\"Total Frames: {cap.get(cv2.CAP_PROP_FRAME_COUNT)}\")\n",
    "\n",
    "# Define codec and create VideoWriter object to save the video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for MP4\n",
    "out = cv2.VideoWriter('output_video_newnew.mp4', fourcc, 30.0, (640, 480))  # Adjust the FPS (30.0) and frame size (640x480) as needed\n",
    "\n",
    "# Known parameters (you need to adjust these values)\n",
    "KNOWN_HEIGHT = 0.6  # height of the traffic light in meters (example)\n",
    "FOCAL_LENGTH = 2623  # Convert focal length to pixels (approx, adjust based on your setup)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Display the original frame\n",
    "    cv2.imshow('Original Frame', frame)  # Show the original frame for debugging\n",
    "\n",
    "    # Resize frame if necessary\n",
    "    frame = cv2.resize(frame, (640, 480))  # Resize to a standard dimension\n",
    "\n",
    "    # Perform inference on the current frame\n",
    "    results = model(frame)\n",
    "\n",
    "    # Loop through the detections\n",
    "    for *box, conf, cls in results.xyxy[0]:  # xyxy format for bounding box\n",
    "        x_min, y_min, x_max, y_max = map(int, box)  # Convert to integer\n",
    "        height_in_pixels = y_max - y_min  # Height of bounding box in pixels\n",
    "\n",
    "        # Calculate the distance\n",
    "        if height_in_pixels > 0:  # Avoid division by zero\n",
    "            distance = (KNOWN_HEIGHT * FOCAL_LENGTH) / height_in_pixels\n",
    "            distance_text = f\"Distance: {distance:.2f} m\"\n",
    "        else:\n",
    "            distance_text = \"Distance: N/A\"\n",
    "\n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (255, 0, 0), 2)  # Bounding box\n",
    "\n",
    "        # Calculate the position for distance text\n",
    "        text_x = x_min\n",
    "        text_y = y_min - 20  # Position above the bounding box\n",
    "\n",
    "        # Draw a background rectangle for the distance text\n",
    "        text_width, text_height = cv2.getTextSize(distance_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]\n",
    "        cv2.rectangle(frame, (text_x, text_y - text_height), (text_x + text_width, text_y + 5), (255, 255, 255), -1)  # White background for text\n",
    "\n",
    "        # Draw the distance text\n",
    "        cv2.putText(frame, distance_text, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "\n",
    "    # Render the detections on the frame\n",
    "    annotated_frame = results.render()[0]  # Rendered frame with bounding boxes and labels\n",
    "\n",
    "    # Combine annotated frame with the original frame for display\n",
    "    combined_frame = cv2.addWeighted(frame, 0.7, annotated_frame, 0.3, 0)\n",
    "\n",
    "    # Write the processed frame to the video file\n",
    "    out.write(combined_frame)  # Save the frame to the output video\n",
    "\n",
    "    # Display the frame with YOLOv5 detections and distance estimations\n",
    "    cv2.imshow('Dashcam Traffic Light Detection and Distance Estimation', combined_frame)\n",
    "\n",
    "    # Exit the video display when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release video capture, VideoWriter, and close display windows\n",
    "cap.release()\n",
    "out.release()  # Release the VideoWriter\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
